主題：基於 BERT 與 DistilBERT 架構之文字情緒分類之專案
一、專題介紹
這是來自於Kaggle的競賽
研究背景與動機：
在現今社群媒體盛行的時代，自動化辨識文字中的情緒特徵對於理解使用者反饋與社群趨勢至關重要。然文字情緒往往具有細微的語意差別，傳統機器學習方法難以精準捕捉上下文關聯。本研究旨在利用 Transformer 架構的預訓練模型，建構一個能自動辨識文字背後情緒類別的自然語言處理（NLP）系統。

任務定義：
本專題任務屬於多類別分類（Multi-class Classification）問題。模型輸入為長度不一的文字序列，輸出則為六種情緒標籤之一：悲傷（sadness）、喜悅（joy）、愛（love）、憤怒（anger）、恐懼（fear）、驚訝（surprise）。

二、資料說明與分析
資料集結構：
本研究採用 dair-ai/emotion 資料集，包含以下結構：

訓練集 (Train)：16,000 筆數據

驗證集 (Validation)：2,000 筆數據

測試集 (Test)：2,000 筆數據

特徵與標籤：
資料包含 text（原始推文文字）與 label（情緒類別代號）兩個欄位。

三、模型設計與實驗設定
模型架構：
本研究對比了兩種主流的 Transformer 模型：

BERT-base-uncased：標準的預訓練 Transformer 模型。

DistilBERT-base-uncased：BERT 的輕量化版本，旨在減少參數規模同時保持效能。

關鍵技術 - 動態填充 (Dynamic Padding)：
為了優化訓練效率，本研究採用 DataCollatorWithPadding 實作動態填充，讓每個 Batch 根據該組資料中最長句子進行補齊，而非固定長度，顯著降低了運算成本。

超參數設定：

學習率 (Learning Rate)：2e-5

批次大小 (Batch Size)：16

訓練週期 (Epochs)：3

權重衰減 (Weight Decay)：0.01

四、實驗結果與分析
經過微調訓練後，模型在測試集上的表現如下表所示：

分析：
實驗結果顯示，輕量化的 DistilBERT 在此任務中表現略優於完整的 BERT 模型。這說明在短文本情緒分類任務中，較精簡的模型結構足以捕捉足夠的語意資訊，且因其參數較少，在微調時可能更容易在有限的 Epochs 下達到較佳的泛化效果。

五、結論
本專題成功建構了基於 Transformer 的情緒分類系統，並透過實驗證實動態填充技術能有效提升訓練效率。雖然目前已達到超過 92.8% 的準確率，未來仍可嘗試導入更先進的架構（如 DeBERTa-V3）或針對資料集中可能存在的類別不平衡情形進行 Loss 權重優化，以進一步提升少數類別的辨識效果。
